{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, f1_score, accuracy_score, recall_score, precision_score\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
      "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
      "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
      "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
      "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
      "3 -0.221929  0.062723  0.061458  123.50      0  \n",
      "4  0.502292  0.219422  0.215153   69.99      0  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"creditcard.csv\")\n",
    "print(df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>...</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>...</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>...</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>...</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9  ...       V21       V22  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  ... -0.018307  0.277838   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425  ... -0.225775 -0.638672   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  ...  0.247998  0.771679   \n",
       "3       1.247203  0.237609  0.377436 -1.387024  ... -0.108300  0.005274   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  ... -0.009431  0.798278   \n",
       "...          ...       ...       ...       ...  ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  ...  0.213454  0.111864   \n",
       "284803  1.058415  0.024330  0.294869  0.584800  ...  0.214205  0.924384   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454  ...  0.232045  0.578229   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087  ...  0.265245  0.800049   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180  ...  0.261057  0.643078   \n",
       "\n",
       "             V23       V24       V25       V26       V27       V28  Amount  \\\n",
       "0      -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053  149.62   \n",
       "1       0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724    2.69   \n",
       "2       0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752  378.66   \n",
       "3      -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458  123.50   \n",
       "4      -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   69.99   \n",
       "...          ...       ...       ...       ...       ...       ...     ...   \n",
       "284802  1.014480 -0.509348  1.436807  0.250034  0.943651  0.823731    0.77   \n",
       "284803  0.012463 -1.016226 -0.606624 -0.395255  0.068472 -0.053527   24.79   \n",
       "284804 -0.037501  0.640134  0.265745 -0.087371  0.004455 -0.026561   67.88   \n",
       "284805 -0.163298  0.123205 -0.569159  0.546668  0.108821  0.104533   10.00   \n",
       "284806  0.376777  0.008797 -0.473649 -0.818267 -0.002415  0.013649  217.00   \n",
       "\n",
       "        Class  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "...       ...  \n",
       "284802      0  \n",
       "284803      0  \n",
       "284804      0  \n",
       "284805      0  \n",
       "284806      0  \n",
       "\n",
       "[284807 rows x 31 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read Data\n",
    "dataset = pd.read_csv(\"creditcard.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Understand the data¶ \n",
    "dataset.Class.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 31)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.165980e-15</td>\n",
       "      <td>3.416908e-16</td>\n",
       "      <td>-1.373150e-15</td>\n",
       "      <td>2.086869e-15</td>\n",
       "      <td>9.604066e-16</td>\n",
       "      <td>1.490107e-15</td>\n",
       "      <td>-5.556467e-16</td>\n",
       "      <td>1.177556e-16</td>\n",
       "      <td>-2.406455e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.656562e-16</td>\n",
       "      <td>-3.444850e-16</td>\n",
       "      <td>2.578648e-16</td>\n",
       "      <td>4.471968e-15</td>\n",
       "      <td>5.340915e-16</td>\n",
       "      <td>1.687098e-15</td>\n",
       "      <td>-3.666453e-16</td>\n",
       "      <td>-1.220404e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.165980e-15  3.416908e-16 -1.373150e-15  2.086869e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   9.604066e-16  1.490107e-15 -5.556467e-16  1.177556e-16 -2.406455e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.656562e-16 -3.444850e-16  2.578648e-16  4.471968e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   5.340915e-16  1.687098e-15 -3.666453e-16 -1.220404e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Fraud vs Non-fraud cases Count')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Exploratory Data Analysis \n",
    "plt.figure(figsize=(10,5))\n",
    "ax = dataset.Class.value_counts().plot(kind = 'bar')\n",
    "plt.xlabel(\"Fraud vs Non-fraud cases\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Fraud vs Non-fraud cases Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "sns.heatmap(data=dataset.corr(), cmap=\"seismic\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Observations from Data-\n",
    "\n",
    "//The data set is very imbalanced with only\n",
    "492 Fraud records and 284315 Non-fraud records.\n",
    "The dataset consists of numerical values from V1 to V28,\n",
    "which looks transformed from real data.\n",
    "But 'Time' and 'Amount' features are not transformed. \n",
    "So we need scaling of dataset. \n",
    "There is no missing value/NaNs in the dataset. \n",
    "Using this data as it is with balancing it might overfit, \n",
    "but for comparison,we will first start with imbalanced data and \n",
    "see the results. Then we will balance the data \n",
    "and check the results again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label data \n",
    "x = dataset.drop([\"Class\"],axis=1)\n",
    "y = dataset[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cluster data using Dimensionality reduction \n",
    "tsne = TSNE(n_components=2 ,random_state=0)\n",
    "x_tsne = tsne.fit_transform(x.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_tsne' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-1b844e0afb82>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mred_patch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmpatches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'#AF0000'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Fraud'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tsne\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_tsne\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'coolwarm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'No Fraud'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tsne\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_tsne\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'coolwarm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Fraud'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinewidths\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m't-SNE'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_tsne' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "blue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\n",
    "red_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\n",
    "plt.scatter(X_tsne[:,0], X_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\n",
    "plt.title('t-SNE')\n",
    "plt.legend(handles=[blue_patch, red_patch])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train and test sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainA = X_train.drop([\"Time\"], axis = 1)\n",
    "X_testA = X_test.drop([\"Time\"], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_trainA)\n",
    "X_test = sc.transform(X_testA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[y_train.values == 0].shape[0])\n",
    "print(y_train[y_train.values == 1].shape[0])\n",
    "print(y_test[y_test.values == 0].shape[0])\n",
    "print(y_test[y_test.values == 1].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictive Analysis on unbalanced data \n",
    "def fit_and_predict(classifier, X_train, X_test, y_train, y_test):\n",
    "    classifier.fit(X_train, y_train)\n",
    "    ypred = classifier.predict(X_test)\n",
    "    \n",
    "    print(\"Accuracy Score:\", accuracy_score(y_test, ypred))\n",
    "    print(\"Recall Score:\", recall_score(y_test, ypred))\n",
    "    print(\"Precision Score:\", precision_score(y_test, ypred))\n",
    "    \n",
    "    print(\"\\n*********Confusion Matrix*********\")\n",
    "    cm = confusion_matrix(y_test, ypred)\n",
    "    print(cm)\n",
    "    fig= plt.figure(figsize=(10,5))\n",
    "    sns.heatmap(cm,cmap=\"coolwarm_r\",annot=True,linewidths=0.5)\n",
    "    plt.title(\"Confusion_matrix\")\n",
    "    plt.xlabel(\"Predicted_class\")\n",
    "    plt.ylabel(\"Real class\")\n",
    "    plt.show()\n",
    "    print(\"\\n*********Classification Report*********\")\n",
    "    print(classification_report(y_test, ypred))\n",
    "    \n",
    "    test_df = pd.DataFrame(X_test, columns = X.columns[1:])\n",
    "    test_df['Actual'] = y_test.values\n",
    "    test_df['Predicted'] = ypred\n",
    "    test_df.head()\n",
    "    tp = test_df[(test_df['Actual'] == 1) & (test_df['Predicted'] == 1)].shape[0]\n",
    "    actual_positive = test_df[(test_df['Actual'] == 1)].shape[0]\n",
    "    print(\"True Positives: \", tp)\n",
    "    print(\"Accuracy for fraud cases: \", (tp / actual_positive))\n",
    "    print(\"ROC AUC Score: \", roc_auc_score(y_test, ypred))\n",
    "    return (y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "y_test, ypred = fit_and_predict(lr, X_train, X_test, y_train, y_test)\n",
    "lr_fp, lr_tp, lr_threshold = roc_curve(y_test, ypred)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decicion tree \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier()\n",
    "y_test, ypred = fit_and_predict(dtree, X_train, X_test, y_train, y_test)\n",
    "dtree_fp, dtree_tp, dtree_threshold = roc_curve(y_test, ypred)\n",
    "dtree_precision, dtree_recall, _ = precision_recall_curve(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random forest \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "y_test, ypred = fit_and_predict(rf, X_train, X_test, y_train, y_test)\n",
    "rf_fp, rf_tp, rf_threshold = roc_curve(y_test, ypred)\n",
    "rf_precision, rf_recall, _ = precision_recall_curve(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adaboost \n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adb = AdaBoostClassifier(n_estimators = 100, random_state = 0)\n",
    "y_test, ypred = fit_and_predict(adb, X_train, X_test, y_train, y_test)\n",
    "adb_fp, adb_tp, adb_threshold = roc_curve(y_test, ypred)\n",
    "adb_precision, adb_recall, _ = precision_recall_curve(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoot \n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier(random_state = 0)\n",
    "y_test, ypred = fit_and_predict(xgb, X_train, X_test, y_train, y_test)\n",
    "xgb_fp, xgb_tp, xgb_threshold = roc_curve(y_test, ypred)\n",
    "xgb_precision, xgb_recall, _ = precision_recall_curve(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate Unbalanced Data\n",
    "#ROC Curve\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot([0, 1], [0, 1], linestyle = \"--\")\n",
    "plt.plot(lr_fp, lr_tp, color=\"red\", label =\"Logistic Regression\")\n",
    "plt.plot(dtree_fp, dtree_tp, color=\"green\", label = \"Decision Tree\")\n",
    "plt.plot(rf_fp, rf_tp, color=\"blue\", label = \"Random Forest\")\n",
    "plt.plot(adb_fp, adb_tp, color=\"orange\", label = \"AdaBoost\")\n",
    "plt.plot(xgb_fp, xgb_tp, color=\"cyan\", label = \"XGBoost\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision-Recall Curve\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.step(lr_recall, lr_precision, marker='.', color='red', label =\"Logistic Regression\")\n",
    "plt.step(dtree_recall, dtree_precision, marker='.', color='green', label = \"Decision Tree\")\n",
    "plt.step(rf_recall, rf_precision, marker='.', color='blue', label = \"Random Forest\")\n",
    "plt.step(adb_recall, adb_precision, marker='.', color='orange', label = \"AdaBoost\")\n",
    "plt.step(xgb_recall, xgb_precision, marker='.', color='cyan', label = \"XGBoost\")\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Balance Data using oversampling method\n",
    "print(X_trainA.shape, y_train.shape)\n",
    "print(X_testA.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_trainA\n",
    "X_train1['Class'] = y_train\n",
    "X_train1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_0_class, X_train_1_class = X_train1.Class.value_counts()\n",
    "print(X_train_0_class, X_train_1_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1_0_df = X_train1[X_train1['Class']==0]\n",
    "X_train1_1_df = X_train1[X_train1['Class']==1]\n",
    "print(X_train1_0_df.shape, X_train1_1_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understand Balanced Data\n",
    "X_train1_1_df = X_train1_1_df.sample(X_train_0_class, replace=True, random_state=0)\n",
    "print(X_train1_0_df.shape, X_train1_1_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = pd.concat([X_train1_0_df, X_train1_1_df])\n",
    "X_train1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train1[X_train1['Class']==0].shape)\n",
    "print(X_train1[X_train1['Class']==1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainB = X_train1.drop(\"Class\", axis =1)\n",
    "y_trainB = X_train1[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_trainB.shape, y_trainB.shape)\n",
    "print(X_testA.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainB = sc.fit_transform(X_trainB)\n",
    "X_testA = sc.transform(X_testA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logistic Regression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "y_test, ypred = fit_and_predict(lr, X_trainB, X_testA, y_trainB, y_test)\n",
    "lr_fp, lr_tp, lr_threshold = roc_curve(y_test, ypred)\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtree = DecisionTreeClassifier()\n",
    "y_test, ypred = fit_and_predict(dtree, X_trainB, X_testA, y_trainB, y_test)\n",
    "dtree_fp, dtree_tp, dtree_threshold = roc_curve(y_test, ypred)\n",
    "dtree_precision, dtree_recall, _ = precision_recall_curve(y_test, ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randomforest \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "y_test, ypred = fit_and_predict(rf, X_trainB, X_testA, y_trainB, y_test)\n",
    "rf_fp, rf_tp, rf_threshold = roc_curve(y_test, ypred)\n",
    "rf_precision, rf_recall, _ = precision_recall_curve(y_test, ypred)\n",
    "Accuracy Score: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adaboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adb = AdaBoostClassifier(n_estimators = 100, random_state = 0)\n",
    "y_test, ypred = fit_and_predict(adb, X_trainB, X_testA, y_trainB, y_test)\n",
    "adb_fp, adb_tp, adb_threshold = roc_curve(y_test, ypred)\n",
    "adb_precision, adb_recall, _ = precision_recall_curve(y_test, ypred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Neural network \n",
    "import keras\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=16, activation=\"relu\", input_dim=29))\n",
    "classifier.add(Dense(units=2, activation=\"softmax\"))\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_trainB, y_trainB, batch_size=10, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred_nn = classifier.predict_classes(X_testA)\n",
    "print(confusion_matrix(y_test, ypred_nn))\n",
    "print(classification_report(y_test, ypred_nn))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, ypred_nn))\n",
    "\n",
    "test_df = pd.DataFrame(X_test, columns = X.columns[1:])\n",
    "test_df['Actual'] = y_test.values\n",
    "test_df['Predicted'] = ypred_nn\n",
    "test_df.head()\n",
    "tp = test_df[(test_df['Actual'] == 1) & (test_df['Predicted'] == 1)].shape[0]\n",
    "actual_positive = test_df[(test_df['Actual'] == 1)].shape[0]\n",
    "print(\"True Positives: \", tp)\n",
    "print(\"Accuracy for fraud cases: \", (tp / actual_positive))\n",
    "print(\"ROC AUC Score: \", roc_auc_score(y_test, ypred_nn))\n",
    "nn_fp, nn_tp, nn_threshold = roc_curve(y_test, ypred_nn)\n",
    "nn_precision, nn_recall, _ = precision_recall_curve(y_test, ypred_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.evaluate(X_testA, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate Balanced Data\n",
    "#ROC Curve\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.plot([0, 1], [0, 1], linestyle = \"--\")\n",
    "plt.plot(lr_fp, lr_tp, color=\"red\", label =\"Logistic Regression\")\n",
    "plt.plot(dtree_fp, dtree_tp, color=\"green\", label = \"Decision Tree\")\n",
    "plt.plot(rf_fp, rf_tp, color=\"blue\", label = \"Random Forest\")\n",
    "plt.plot(adb_fp, adb_tp, color=\"orange\", label = \"AdaBoost\")\n",
    "plt.plot(xgb_fp, xgb_tp, color=\"cyan\", label = \"XGBoost\")\n",
    "plt.plot(nn_fp, nn_tp, color=\"purple\", label = \"Neural Networks\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve\")\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall Curve\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.step(lr_recall, lr_precision, marker='.', color='red', label =\"Logistic Regression\")\n",
    "plt.step(dtree_recall, dtree_precision, marker='.', color='green', label = \"Decision Tree\")\n",
    "plt.step(rf_recall, rf_precision, marker='.', color='blue', label = \"Random Forest\")\n",
    "plt.step(adb_recall, adb_precision, marker='.', color='orange', label = \"AdaBoost\")\n",
    "plt.step(xgb_recall, xgb_precision, marker='.', color='cyan', label = \"XGBoost\")\n",
    "plt.step(nn_recall, nn_precision, marker='.', color=\"purple\", label = \"Neural Networks\")\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall curve')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance\n",
    "X_trainA.columns[:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance_df = pd.DataFrame(X_trainA.columns[:-1], columns=[\"Feature\"])\n",
    "feature_importance_df[\"Importance\"] = rf.feature_importances_\n",
    "feature_importance_df.sort_values('Importance', ascending=False, inplace=True)\n",
    "feature_importance_df = feature_importance_df.head(20)\n",
    "feature_importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "ax = feature_importance_df['Feature']\n",
    "plt.bar(range(feature_importance_df.shape[0]), feature_importance_df['Importance']*100)\n",
    "plt.xticks(range(feature_importance_df.shape[0]), feature_importance_df['Feature'], rotation = 20)\n",
    "plt.xlabel(\"Features\")\n",
    "plt.ylabel(\"Importance\")\n",
    "plt.title(\"Plot Feature Importances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Conclusion\n",
    "\n",
    "After investigating and visualizing through the data, we found that the data is unbalanced. So we balanced the data.\n",
    "After balancing training data, we trainied it and these were the results-\n",
    "\n",
    "Logistic Regression is giving 98% accuracy, 91% recall and 6% precision. Accuracy for fraud cases is 91% and ROC AUC Score is 94%\n",
    "Decision Tree is giving 99% accuracy, 69% recall and 76% precision. Accuracy for fraud cases is 69% and ROC AUC Score is 85%\n",
    "Random Forest is giving 99% accuracy, 78% recall and 95% precision. Accuracy for fraud cases is 78% and ROC AUC Score is 89%\n",
    "AdaBoost is giving 99% accuracy, 86 % recall and 18% precision. Accuracy for fraud cases is 86% and ROC AUC Score is 93%\n",
    "XGBoost is giving 99% accuracy, 83 % recall and 92% precision. Accuracy for fraud cases is 83% and ROC AUC Score is 91%\n",
    "Neural Networks is giving 99% accuracy, 82 % recall and 44% precision. Accuracy for fraud cases is 82% and ROC AUC Score is 91%\n",
    "\n",
    "With Neural Networks on balanced dataset, \n",
    "its giving 86% accuracy on Fraud Cases. \n",
    "But still for Fraud cases the winner is Logistic Regression with 91% accuracy on Fraud Cases. But logistic regression is \n",
    "giving more False Positives and the precision is less.\n",
    "Boosting model - Adaboost is next consideration with accuracy of 93%. If we want to give precision the preference, \n",
    "then XGBoost can be considered,\n",
    "and accuracy can be improved twiking some parameters.\n",
    "\n",
    "Hence it depemds on the accuracy parameter we want to consider,\n",
    "but for the unbalanced data most of the time consideration is \n",
    "given to ROC AUC score so Logistic Regression is winner here, \n",
    "as its giving most True Positives. \n",
    "Specially, in cases like fraud transactions False Positives \n",
    "can be fine."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
